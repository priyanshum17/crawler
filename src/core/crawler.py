import json
import asyncio
from datetime import datetime, timezone
from pathlib import Path
import aiohttp
import hashlib
from pathlib import Path
from urllib.parse import urlparse

from src.core.parser import parse_html
from src.core.database import init_db

SEED_URL = "https://cc.gatech.edu"
MAX_PAGES = 1_000
USER_AGENT = "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36"


class Crawler:
    def __init__(self):
        self.frontier = asyncio.Queue()
        self.frontier.put_nowait(SEED_URL)
        self.seen = set([SEED_URL])
        self.pages_done = 0
        self.db = None

    async def start(self):
        self.db = await init_db()

        async with aiohttp.ClientSession(headers={"User-Agent": USER_AGENT}) as sess:
            workers = [asyncio.create_task(self.worker(sess)) for _ in range(10)]
            await self.frontier.join()

            for w in workers:
                w.cancel()
            await asyncio.gather(*workers, return_exceptions=True)

        await self.db.close()

    async def worker(self, sess):
        while True:
            try:
                url = await asyncio.wait_for(self.frontier.get(), timeout=5)
            except asyncio.TimeoutError:
                break

            if self.pages_done >= MAX_PAGES:
                self.frontier.task_done()
                continue

            try:
                await self.handle_url(url, sess)
            finally:
                self.frontier.task_done()

    async def handle_url(self, url, sess):
        try:
            async with sess.get(url, allow_redirects=True) as resp:
                status = resp.status
                html = await resp.read()
                content_type = resp.headers.get("Content-Type", "")
        except Exception as e:
            await self.db.execute(
                "INSERT OR IGNORE INTO failures VALUES (?,?,?)",
                (url, str(e), datetime.now(timezone.utc).isoformat()),
            )
            await self.db.commit()
            return

        raw_path = save_raw(Path("data/raw/"), html)

        if "text/html" not in content_type:
            await self.db.execute(
                "INSERT OR IGNORE INTO pages(url,http_status,fetch_time,content_type,raw_html_path)"
                "VALUES(?,?,?,?,?)",
                (
                    url,
                    status,
                    datetime.now(timezone.utc).isoformat(),
                    content_type,
                    raw_path,
                ),
            )
            await self.db.commit()
            return

        rec = parse_html(url, html)
        rec["raw_html_path"] = raw_path

        cols = ",".join(rec.keys())
        placeholders = ",".join("?" * len(rec))
        await self.db.execute(
            f"INSERT OR IGNORE INTO pages({cols}) VALUES({placeholders})",
            tuple(rec.values()),
        )
        await self.db.commit()

        self.pages_done += 1
        if self.pages_done >= MAX_PAGES:
            return

        for link in json.loads(rec.get("outbound_links", "[]")):
            if in_scope(link) and link not in self.seen:
                self.seen.add(link)
                self.frontier.put_nowait(link)


def save_raw(raw_dir: Path, html_bytes: bytes) -> str:
    """
    Save raw HTML bytes to a file in `raw_dir` and return the file path as string.

    The filename is generated by hashing the content to avoid duplicates and illegal chars.
    """
    raw_dir.mkdir(parents=True, exist_ok=True)
    filename = hashlib.sha256(html_bytes).hexdigest() + ".html"
    file_path = raw_dir / filename

    with open(file_path, "wb") as f:
        f.write(html_bytes)

    return str(file_path)


def in_scope(u: str) -> bool:
    """Return True if URL is inside cc.gatech.edu (sub-domains allowed)."""
    return urlparse(u).netloc.endswith("cc.gatech.edu")
